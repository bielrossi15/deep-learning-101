# ACTIVATION FUNCTIONS

**Sigmoid function is a activation function, but we can have others**

g = othe activation function

example = < tanh function is better almost always, but sigmoid function is better sometimes in binary classifications, so we can use tanh in hidden layer and sigmoid in output, but the default activation function is RELU > 

< The function with the least amount of derivatives close to zero is the mos used >

 Z[1] = W[1]*X + b[1]
 A[1] = g(Z[1]) 

 Z[2] = W[2]*A[1] + b[2]
 A[2] = g(Z[2])















